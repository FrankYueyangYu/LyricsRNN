{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import csv\r\n",
    "import numpy as np \r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "\r\n",
    "from bs4 import BeautifulSoup as bs\r\n",
    "import json\r\n",
    "import re\r\n",
    "import requests\r\n",
    "from config import access_token, client_secret, client_id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# lyricsgenius wrapper to get Top 100 Snoop lyrics\r\n",
    "import lyricsgenius\r\n",
    "genius = lyricsgenius.Genius(access_token, sleep_time=0.01, verbose=False, remove_section_headers = True, excluded_terms = [\"(Remix)\", \"(Live)\"], skip_non_songs= True)\r\n",
    "artist = genius.search_artist('Snoop Dogg', max_songs = 100, get_full_info= False)\r\n",
    "artist.save_lyrics()\r\n",
    "\r\n",
    "# lyricsgenius wrapper to get Top 100 Taylor Swift lyrics\r\n",
    "genius2 = lyricsgenius.Genius(access_token, sleep_time=0.01, verbose=False, remove_section_headers = True, excluded_terms = [\"(Remix)\", \"(Live)\"], skip_non_songs= True)\r\n",
    "artist2 = genius2.search_artist('Taylor Swift', max_songs = 100, get_full_info= False)\r\n",
    "artist2.save_lyrics()\r\n",
    "\r\n",
    "# lyricsgenius wrapper to get Top 100 Ed Sheeran lyrics\r\n",
    "genius3 = lyricsgenius.Genius(access_token, sleep_time=0.01, verbose=False, remove_section_headers = True, excluded_terms = [\"(Remix)\", \"(Live)\"], skip_non_songs= True)\r\n",
    "artist3 = genius3.search_artist('Ed Sheeran', max_songs = 100, get_full_info= False)\r\n",
    "artist3.save_lyrics()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open ('Lyrics_SnoopDogg.json') as json_file:\r\n",
    "    data = json.load(json_file)\r\n",
    "\r\n",
    "data['songs'][0]['lyrics']\r\n",
    "\r\n",
    "# read Json, extract to df \r\n",
    "title = []\r\n",
    "url = []\r\n",
    "lyrics = []\r\n",
    "for i in data['songs']:\r\n",
    "    title.append(i['title']),\r\n",
    "    url.append(i['url']),\r\n",
    "    lyrics.append(i['lyrics'])\r\n",
    "\r\n",
    "snoop_songs = pd.DataFrame(zip(title,url,lyrics),columns= ['Title','URL','Lyrics'])\r\n",
    "\r\n",
    "# # drop a row since its a comic not song\r\n",
    "# snoop_songs.drop([len(snoop_songs)-2],inplace=True).reset_index(drop=True)\r\n",
    "\r\n",
    "\r\n",
    "# replace line break, backward slash, random string at the end, texts within parenthesis\r\n",
    "repl = {'\\n':' ', '\\\\' : '', 'EmbedShare URLCopyEmbedCopy' : '' }\r\n",
    "for key, value in repl.items():\r\n",
    "    snoop_songs['Lyrics'] = snoop_songs['Lyrics'].str.replace(key,value)\r\n",
    "    \r\n",
    "for i in range(len(snoop_songs['Lyrics'])):\r\n",
    "    snoop_songs['Lyrics'][i]= re.sub(r\"\\([^()]*\\)\", \"\", snoop_songs['Lyrics'][i])\r\n",
    "    # wrapper had an issue, some lyrics have extra number digit and embed string added to the end. \r\n",
    "    while snoop_songs['Lyrics'][i][-1].isdigit() == True:\r\n",
    "        snoop_songs['Lyrics'][i] = snoop_songs['Lyrics'][i][:-1]\r\n",
    "\r\n",
    "    else :\r\n",
    "        pass\r\n",
    "    \r\n",
    "    \r\n",
    "snoop_songs.to_csv(\"Snoop_Lyrics.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open ('Lyrics_TaylorSwift.json') as json_file:\r\n",
    "    data = json.load(json_file)\r\n",
    "\r\n",
    "data['songs'][0]['lyrics']\r\n",
    "\r\n",
    "# read Json, extract to df \r\n",
    "title = []\r\n",
    "url = []\r\n",
    "lyrics = []\r\n",
    "for i in data['songs']:\r\n",
    "    title.append(i['title']),\r\n",
    "    url.append(i['url']),\r\n",
    "    lyrics.append(i['lyrics'])\r\n",
    "    \r\n",
    "taytay_songs = pd.DataFrame(zip(title,url,lyrics),columns= ['Title','URL','Lyrics'])\r\n",
    "\r\n",
    "\r\n",
    "# replace line break, backward slash, random string at the end, texts within parenthesis\r\n",
    "repl = {'\\n':' ', '\\\\' : '', 'EmbedShare URLCopyEmbedCopy' : '' }\r\n",
    "for key, value in repl.items():\r\n",
    "    taytay_songs['Lyrics'] = taytay_songs['Lyrics'].str.replace(key,value)\r\n",
    "    \r\n",
    "for i in range(len(taytay_songs['Lyrics'])):\r\n",
    "    taytay_songs['Lyrics'][i]= re.sub(r\"\\([^()]*\\)\", \"\", taytay_songs['Lyrics'][i])\r\n",
    "    # wrapper had an issue, some lyrics have extra number digit and embed string added to the end. \r\n",
    "    while taytay_songs['Lyrics'][i][-1].isdigit() == True:\r\n",
    "        taytay_songs['Lyrics'][i] = taytay_songs['Lyrics'][i][:-1]\r\n",
    "\r\n",
    "    else :\r\n",
    "        pass\r\n",
    "    \r\n",
    "taytay_songs.to_csv(\"Taytay_Lyrics.csv\", index=False)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open ('Lyrics_EdSheeran.json') as json_file:\r\n",
    "    data = json.load(json_file)\r\n",
    "\r\n",
    "data['songs'][0]['lyrics']\r\n",
    "\r\n",
    "# read Json, extract to df \r\n",
    "title = []\r\n",
    "url = []\r\n",
    "lyrics = []\r\n",
    "for i in data['songs']:\r\n",
    "    title.append(i['title']),\r\n",
    "    url.append(i['url']),\r\n",
    "    lyrics.append(i['lyrics'])\r\n",
    "\r\n",
    "ed_songs = pd.DataFrame(zip(title,url,lyrics),columns= ['Title','URL','Lyrics'])\r\n",
    "\r\n",
    "# replace line break, backward slash, random string at the end, texts within parenthesis\r\n",
    "repl = {'\\n':' ', '\\\\' : '', 'EmbedShare URLCopyEmbedCopy' : '' }\r\n",
    "for key, value in repl.items():\r\n",
    "    ed_songs['Lyrics'] = ed_songs['Lyrics'].str.replace(key,value)\r\n",
    "    \r\n",
    "for i in range(len(ed_songs['Lyrics'])):\r\n",
    "    # wrapper had an issue, some lyrics have extra number digit and embed string added to the end. \r\n",
    "    while ed_songs['Lyrics'][i][-1].isdigit() == True:\r\n",
    "        ed_songs['Lyrics'][i] = ed_songs['Lyrics'][i][:-1]\r\n",
    "\r\n",
    "    else :\r\n",
    "        pass\r\n",
    "    \r\n",
    "    \r\n",
    "ed_songs.to_csv(\"Ed_Lyrics.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "snoop_df = pd.read_csv(\"Snoop_Lyrics.csv\")\r\n",
    "snoop_df['LowLyrics'] = snoop_df['Lyrics'].str.lower()\r\n",
    "snoop_df"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\frank\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                Title  \\\n",
       "0                                       Gin and Juice   \n",
       "1                               Drop It Like It’s Hot   \n",
       "2        Ain’t No Fun (If the Homies Can’t Have None)   \n",
       "3                          Who Am I (What’s My Name)?   \n",
       "4   Murder Was the Case (Death After Visualizing E...   \n",
       "..                                                ...   \n",
       "94                                           Buck ’Em   \n",
       "95                                       Just Dippin’   \n",
       "96                    On TDE: “They’re Not Death Row”   \n",
       "97                                      Midnight Love   \n",
       "98                                            Edibles   \n",
       "\n",
       "                                                  URL  \\\n",
       "0   https://genius.com/Snoop-dogg-gin-and-juice-ly...   \n",
       "1   https://genius.com/Snoop-dogg-drop-it-like-its...   \n",
       "2   https://genius.com/Snoop-dogg-aint-no-fun-if-t...   \n",
       "3   https://genius.com/Snoop-dogg-who-am-i-whats-m...   \n",
       "4   https://genius.com/Snoop-dogg-murder-was-the-c...   \n",
       "..                                                ...   \n",
       "94       https://genius.com/Snoop-dogg-buck-em-lyrics   \n",
       "95   https://genius.com/Snoop-dogg-just-dippin-lyrics   \n",
       "96  https://genius.com/Snoop-dogg-on-tde-theyre-no...   \n",
       "97  https://genius.com/Snoop-dogg-midnight-love-ly...   \n",
       "98       https://genius.com/Snoop-dogg-edibles-lyrics   \n",
       "\n",
       "                                               Lyrics  \\\n",
       "0    Ha-ha-ha, I'm serious, nigga One of y'all nig...   \n",
       "1   Snoop Snoop  When the pimp's in the crib, ma D...   \n",
       "2   You're back now at the jack-off hour This is D...   \n",
       "3   Eee-ya-ya-ya-ya-yah The Dogg Pound's in the ho...   \n",
       "4    Aye, aye,  JC 'Sup Enron Ain't that Snoop Dog...   \n",
       "..                                                ...   \n",
       "94   Damn, this shit sound low than a motherfucker...   \n",
       "95  Yeah, uh  Wanna say what's up to all them ride...   \n",
       "96  No, they're not the new Death Row Because TDE ...   \n",
       "97  Midnight love Midnight love Midnight love Midn...   \n",
       "98  And once it all dissolves  And it mixes with t...   \n",
       "\n",
       "                                            LowLyrics  \n",
       "0    ha-ha-ha, i'm serious, nigga one of y'all nig...  \n",
       "1   snoop snoop  when the pimp's in the crib, ma d...  \n",
       "2   you're back now at the jack-off hour this is d...  \n",
       "3   eee-ya-ya-ya-ya-yah the dogg pound's in the ho...  \n",
       "4    aye, aye,  jc 'sup enron ain't that snoop dog...  \n",
       "..                                                ...  \n",
       "94   damn, this shit sound low than a motherfucker...  \n",
       "95  yeah, uh  wanna say what's up to all them ride...  \n",
       "96  no, they're not the new death row because tde ...  \n",
       "97  midnight love midnight love midnight love midn...  \n",
       "98  and once it all dissolves  and it mixes with t...  \n",
       "\n",
       "[99 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>LowLyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gin and Juice</td>\n",
       "      <td>https://genius.com/Snoop-dogg-gin-and-juice-ly...</td>\n",
       "      <td>Ha-ha-ha, I'm serious, nigga One of y'all nig...</td>\n",
       "      <td>ha-ha-ha, i'm serious, nigga one of y'all nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drop It Like It’s Hot</td>\n",
       "      <td>https://genius.com/Snoop-dogg-drop-it-like-its...</td>\n",
       "      <td>Snoop Snoop  When the pimp's in the crib, ma D...</td>\n",
       "      <td>snoop snoop  when the pimp's in the crib, ma d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ain’t No Fun (If the Homies Can’t Have None)</td>\n",
       "      <td>https://genius.com/Snoop-dogg-aint-no-fun-if-t...</td>\n",
       "      <td>You're back now at the jack-off hour This is D...</td>\n",
       "      <td>you're back now at the jack-off hour this is d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who Am I (What’s My Name)?</td>\n",
       "      <td>https://genius.com/Snoop-dogg-who-am-i-whats-m...</td>\n",
       "      <td>Eee-ya-ya-ya-ya-yah The Dogg Pound's in the ho...</td>\n",
       "      <td>eee-ya-ya-ya-ya-yah the dogg pound's in the ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Murder Was the Case (Death After Visualizing E...</td>\n",
       "      <td>https://genius.com/Snoop-dogg-murder-was-the-c...</td>\n",
       "      <td>Aye, aye,  JC 'Sup Enron Ain't that Snoop Dog...</td>\n",
       "      <td>aye, aye,  jc 'sup enron ain't that snoop dog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Buck ’Em</td>\n",
       "      <td>https://genius.com/Snoop-dogg-buck-em-lyrics</td>\n",
       "      <td>Damn, this shit sound low than a motherfucker...</td>\n",
       "      <td>damn, this shit sound low than a motherfucker...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Just Dippin’</td>\n",
       "      <td>https://genius.com/Snoop-dogg-just-dippin-lyrics</td>\n",
       "      <td>Yeah, uh  Wanna say what's up to all them ride...</td>\n",
       "      <td>yeah, uh  wanna say what's up to all them ride...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>On TDE: “They’re Not Death Row”</td>\n",
       "      <td>https://genius.com/Snoop-dogg-on-tde-theyre-no...</td>\n",
       "      <td>No, they're not the new Death Row Because TDE ...</td>\n",
       "      <td>no, they're not the new death row because tde ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Midnight Love</td>\n",
       "      <td>https://genius.com/Snoop-dogg-midnight-love-ly...</td>\n",
       "      <td>Midnight love Midnight love Midnight love Midn...</td>\n",
       "      <td>midnight love midnight love midnight love midn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Edibles</td>\n",
       "      <td>https://genius.com/Snoop-dogg-edibles-lyrics</td>\n",
       "      <td>And once it all dissolves  And it mixes with t...</td>\n",
       "      <td>and once it all dissolves  and it mixes with t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from nltk.corpus import stopwords\r\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "import pyLDAvis \r\n",
    "import pyLDAvis.sklearn\r\n",
    "from sklearn.manifold import TSNE"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# LDA topics / Snoop\r\n",
    "Snoop_songs = []\r\n",
    "for ly in snoop_df['LowLyrics']:\r\n",
    "    Snoop_songs.append(ly)\r\n",
    "\r\n",
    "sp_vectorizer = CountVectorizer(stop_words='english')\r\n",
    "sp = sp_vectorizer.fit_transform(Snoop_songs)\r\n",
    "sp_words = sp_vectorizer.get_feature_names()\r\n",
    "\r\n",
    "\r\n",
    "lda_model = LatentDirichletAllocation( max_iter=500, \r\n",
    "                                learning_method='online', learning_offset=50.,random_state=0)\r\n",
    "lda_model.fit(sp)\r\n",
    "lda_W = lda_model.transform(sp)\r\n",
    "lda_H = lda_model.components_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "top_words = 5\r\n",
    "topic_summaries=[]\r\n",
    "def LDA_display(topic_matrix,feature_names, no_top_words):\r\n",
    "    \"\"\" This function prints the different topic numbers with the \r\n",
    "    four most common words in every topic\"\"\"\r\n",
    "    for topic_idx, topic in enumerate(topic_matrix):\r\n",
    "        print (\"Topic {}\".format(topic_idx))\r\n",
    "        x= (\" \".join([feature_names[i]\r\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\r\n",
    "        print(x)\r\n",
    "        topic_summaries.append(x)\r\n",
    "\r\n",
    "LDA_display(lda_H,sp_words,top_words)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic 0\n",
      "like nigga don got niggas\n",
      "Topic 1\n",
      "come running world rip su\n",
      "Topic 2\n",
      "nah life street make don\n",
      "Topic 3\n",
      "da dogg doggy di blam\n",
      "Topic 4\n",
      "legend motherfucking look ain reflection\n",
      "Topic 5\n",
      "nah ain just dogg don\n",
      "Topic 6\n",
      "em like got want drip\n",
      "Topic 7\n",
      "dutchie pass 20 like left\n",
      "Topic 8\n",
      "imagine gang like sta away\n",
      "Topic 9\n",
      "bitch don play need player\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\frank\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# LDA topics / Taylor Swift\r\n",
    "taytay_df = pd.read_csv(\"Taytay_Lyrics.csv\")\r\n",
    "taytay_df['LowLyrics'] = taytay_df['Lyrics'].str.lower()\r\n",
    "taytay_songs = []\r\n",
    "for ly in taytay_df['LowLyrics']:\r\n",
    "    taytay_songs.append(ly)\r\n",
    "\r\n",
    "sp_vectorizer2 = CountVectorizer(stop_words='english')\r\n",
    "sp2 = sp_vectorizer2.fit_transform(taytay_songs)\r\n",
    "sp_words2 = sp_vectorizer2.get_feature_names()\r\n",
    "\r\n",
    "\r\n",
    "lda_model2 = LatentDirichletAllocation( max_iter=500, \r\n",
    "                                learning_method='online', learning_offset=50.,random_state=0)\r\n",
    "lda_model2.fit(sp2)\r\n",
    "lda_W2 = lda_model2.transform(sp2)\r\n",
    "lda_H2 = lda_model2.components_\r\n",
    "\r\n",
    "\r\n",
    "top_words = 5\r\n",
    "topic_summaries2=[]\r\n",
    "def LDA_display(topic_matrix,feature_names, no_top_words):\r\n",
    "    \"\"\" This function prints the different topic numbers with the \r\n",
    "    four most common words in every topic\"\"\"\r\n",
    "    for topic_idx, topic in enumerate(topic_matrix):\r\n",
    "        print (\"Topic {}\".format(topic_idx))\r\n",
    "        x= (\" \".join([feature_names[i]\r\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\r\n",
    "        print(x)\r\n",
    "        topic_summaries2.append(x)\r\n",
    "\r\n",
    "LDA_display(lda_H2,sp_words2,top_words)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\frank\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic 0\n",
      "ooh shake isn know trouble\n",
      "Topic 1\n",
      "think street walk cornelia stay\n",
      "Topic 2\n",
      "like love want don know\n",
      "Topic 3\n",
      "man didn dead stay died\n",
      "Topic 4\n",
      "clear woods mr good remember\n",
      "Topic 5\n",
      "di da good car getaway\n",
      "Topic 6\n",
      "oh daylight love don ll\n",
      "Topic 7\n",
      "oh just like know dancing\n",
      "Topic 8\n",
      "know ve don ll got\n",
      "Topic 9\n",
      "look just ll don time\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "# LDA topics / Ed\r\n",
    "ed_df = pd.read_csv(\"Ed_Lyrics.csv\")\r\n",
    "ed_df['LowLyrics'] = ed_df['Lyrics'].str.lower()\r\n",
    "ed_songs = []\r\n",
    "for ly in ed_df['LowLyrics']:\r\n",
    "    ed_songs.append(ly)\r\n",
    "\r\n",
    "sp_vectorizer3 = CountVectorizer(stop_words='english')\r\n",
    "sp3 = sp_vectorizer3.fit_transform(ed_songs)\r\n",
    "sp_words3 = sp_vectorizer3.get_feature_names()\r\n",
    "\r\n",
    "\r\n",
    "lda_model3 = LatentDirichletAllocation( max_iter=500, \r\n",
    "                                learning_method='online', learning_offset=50.,random_state=0)\r\n",
    "lda_model3.fit(sp3)\r\n",
    "lda_W3 = lda_model3.transform(sp3)\r\n",
    "lda_H3 = lda_model3.components_\r\n",
    "\r\n",
    "\r\n",
    "top_words = 5\r\n",
    "topic_summaries3=[]\r\n",
    "def LDA_display(topic_matrix,feature_names, no_top_words):\r\n",
    "    \"\"\" This function prints the different topic numbers with the \r\n",
    "    four most common words in every topic\"\"\"\r\n",
    "    for topic_idx, topic in enumerate(topic_matrix):\r\n",
    "        print (\"Topic {}\".format(topic_idx))\r\n",
    "        x= (\" \".join([feature_names[i]\r\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\r\n",
    "        print(x)\r\n",
    "        topic_summaries3.append(x)\r\n",
    "\r\n",
    "LDA_display(lda_H3,sp_words3,top_words)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\frank\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic 0\n",
      "galway love girl want know\n",
      "Topic 1\n",
      "rain make ooh hopeless place\n",
      "Topic 2\n",
      "ay going hey yeah mother\n",
      "Topic 3\n",
      "love know ll don like\n",
      "Topic 4\n",
      "ye lo lonely wo nooma\n",
      "Topic 5\n",
      "time ll like little border\n",
      "Topic 6\n",
      "afterglow hold street home city\n",
      "Topic 7\n",
      "like cross glue oh let\n",
      "Topic 8\n",
      "oh love ooh feel lover\n",
      "Topic 9\n",
      "ll fall eyes shirtsleeves just\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\r\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.optimizers import Adam\r\n",
    "\r\n",
    "from tensorflow.keras.regularizers import Regularizer\r\n",
    "\r\n",
    "import tensorflow.keras.utils as ku "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\frank\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "with open ('Lyrics_TaylorSwift.json') as json_file:\r\n",
    "    data = json.load(json_file)\r\n",
    "\r\n",
    "# read Json, extract to df \r\n",
    "title = []\r\n",
    "url = []\r\n",
    "lyrics = []\r\n",
    "for i in data['songs']:\r\n",
    "    title.append(i['title']),\r\n",
    "    url.append(i['url']),\r\n",
    "    lyrics.append(i['lyrics'])\r\n",
    "    \r\n",
    "t_songs = pd.DataFrame(zip(title,url,lyrics),columns= ['Title','URL','Lyrics'])\r\n",
    "\r\n",
    "\r\n",
    "# replace line break, backward slash, random string at the end, texts within parenthesis\r\n",
    "repl = {'EmbedShare URLCopyEmbedCopy' : '' }\r\n",
    "for key, value in repl.items():\r\n",
    "    t_songs['Lyrics'] = t_songs['Lyrics'].str.replace(key,value)\r\n",
    "    \r\n",
    "for i in range(len(t_songs['Lyrics'])):\r\n",
    "    while t_songs['Lyrics'][i][-1].isdigit() == True:\r\n",
    "        t_songs['Lyrics'][i] = t_songs['Lyrics'][i][:-1]\r\n",
    "\r\n",
    "    else :\r\n",
    "        pass\r\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\frank\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "t_combined = []\r\n",
    "for i in range(len(t_songs['Lyrics'])):\r\n",
    "    t_combined.append(t_songs['Lyrics'][i].lower())\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\frank\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "corpus = t_combined\r\n",
    "corpus = list(set(corpus))\r\n",
    "\r\n",
    "for i in range(len(corpus)):\r\n",
    "    sentence = corpus[i]\r\n",
    "    sentence = \"startsentence \" + sentence + \" endsentence\"\r\n",
    "    corpus[i] = sentence"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\frank\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "corpus[1]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\frank\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"startsentence hey, dorothea, do you ever stop and think about me?\\nwhen we were younger down in the park\\nhoney, making a lark of the misery\\nyou got shiny friends since you left town\\na tiny screen's the only place i see you now\\nand i got nothing but well wishes for ya\\n\\nooh, this place is the same as it ever was\\nooh, but you won't like it that way\\n\\nit's never too late to come back to my side\\nthe stars in your eyes shined brighter in tupelo\\nand if you're ever tired of bеing known for who you know\\nyou know that you'll always know me, dorothea (uh-uh)\\ndorothea (ah-ah)\\n\\nooh, you'rе a queen sellin' dreams, sellin' makeup and magazines\\nooh, from you, i'd buy anything\\n\\nhey, dorothea, do you ever stop and think about me?\\nwhen it was calmer, skipping the prom just to piss off your mom and her pageant schemes\\nand damn, dorothea, they all wanna be ya\\nbut are you still the same soul i met under the bleachers? well\\nooh, i guess i'll never know\\nooh, and you'll go on with the show\\n\\nbut it's never too late to come back to my side\\nthe stars in your eyes shined brighter in tupelo\\nand if you're ever tired of being known for who you know\\nyou know, you'll always know me, dorothea (uh-uh)\\ndorothea (ah-ah)\\n\\nooh, ooh\\nooh-woo-ooh-ooh-ooh, ooh-ooh-ooh-ooh\\nooh, ooh\\nooh-woo-ooh-ooh-ooh, ooh-ooh-ooh\\ndorothea (ah-ah-ah)\\nah-ah\\nooh endsentence\""
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "tokenizer = Tokenizer()\r\n",
    "\r\n",
    "tokenizer.fit_on_texts(corpus)\r\n",
    "total_words = len(tokenizer.word_index) + 1\r\n",
    "\r\n",
    "input_sequences = []\r\n",
    "for line in corpus:\r\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\r\n",
    "    for i in range(1, len(token_list)):\r\n",
    "        n_gram_sequence = token_list[:i+1]\r\n",
    "        input_sequences.append(n_gram_sequence)\r\n",
    "\r\n",
    "\r\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\r\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\r\n",
    "\r\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\r\n",
    "\r\n",
    "label = ku.to_categorical(label, num_classes=total_words)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\frank\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "model = Sequential()\r\n",
    "model.add(Embedding(total_words, 50, input_length=max_sequence_len-1))  \r\n",
    "model.add(Bidirectional(LSTM(150, return_sequences=True)))  \r\n",
    "model.add(Dropout(0.2)) \r\n",
    "model.add(LSTM(100))  \r\n",
    "model.add(Dense(total_words/2, activation='relu'))  \r\n",
    "model.add(Dense(total_words, activation='softmax'))  \r\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')  \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\frank\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "history = model.fit(predictors, label, epochs= 100, verbose=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\frank\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "  46/1200 [>.............................] - ETA: 2:45:29 - loss: 7.0041 - accuracy: 0.0353"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_lyrics(seed_text):\r\n",
    "    ans = True\r\n",
    "    seed_text = \"startsentence \" + seed_text \r\n",
    "    while True:\r\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\r\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\r\n",
    "        predicted = np.argmax(model.predict(token_list, verbose=0), axis = -1)\r\n",
    "        output_word = \"\"\r\n",
    "        for word, index in tokenizer.word_index.items():\r\n",
    "            if index == predicted:\r\n",
    "                output_word = word\r\n",
    "                break\r\n",
    "        if output_word == \"endsentence\":\r\n",
    "            new_sentence_words = seed_text.split()[1:]\r\n",
    "            seed_text = ' '.join(new_sentence_words)\r\n",
    "            print(seed_text)\r\n",
    "            ans = False\r\n",
    "            break\r\n",
    "        seed_text += \" \" + output_word\r\n",
    "    if ans == True:\r\n",
    "        new_sentence_words = seed_text.split()[1:]\r\n",
    "        seed_text = ' '.join(new_sentence_words)\r\n",
    "        print(seed_text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "create_lyrics('I was')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "16962a81fd87ca12fc8229dc5892b701c46750bc2c618b006ecbbf76002778a9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}